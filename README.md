# 📊 Word Count Application Using Apache Spark
## 📝 Overview
This project focuses on processing text data using Apache Spark, specifically through the use of Resilient Distributed Datasets (RDDs). The main objective is to build a word count application that analyzes the frequency of words in a text file containing the English Bible from the Corpus Canterbury.

## 🎯 Objectives

Part 1: Create an RDD and a Pair RDD.
Part 2: Count words using a Pair RDD.
Part 3: Find individual words and calculate their average frequency of occurrence.
Part 4: Apply the developed functionalities to a text file.
Part 5: Calculate various statistical metrics related to word frequencies.

## 🛠️ Technologies Used

Apache Spark (Python API)
Jupyter Notebooks

## 🚀  Getting Started

Clone the repository to your local machine.
Ensure you have Apache Spark installed along with the necessary Python libraries.
Open the Jupyter Notebook and run the provided code cells sequentially.

## ✨ Features

Counts the total number of distinct words with specific criteria (e.g., 'a' in the third position).
Displays a sample of the longest words and counts how many have three or more vowels.
Scalable to analyze larger datasets, such as Wikipedia.

## 📁 Files

usuarioUOC_22.519_PEC1_WordCount_ES.ipynb: Jupyter Notebook containing the implementation of the word count application.

### ▶️ Usage
To execute the application, run the Jupyter Notebook cells in order. The output will display the total counts, samples of words, and other relevant statistics
